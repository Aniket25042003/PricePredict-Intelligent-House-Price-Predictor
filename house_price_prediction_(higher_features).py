# -*- coding: utf-8 -*-
"""House Price Prediction (Higher Features).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14e14yeVu0Fo4wUneq8kHWkElHXK0V0Xa
"""

# Importing Necessary Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

# Mount Google Drive and load dataset
from google.colab import drive
drive.mount('/content/drive')
train_df = pd.read_csv('/content/drive/MyDrive/house-prices-advanced-regression-techniques/train.csv')
test_df = pd.read_csv('/content/drive/MyDrive/house-prices-advanced-regression-techniques/test.csv')

# Data Preprocessing
# Separate numeric and categorical columns (excluding the target variable 'SalePrice' from numeric columns)
numeric_cols = train_df.select_dtypes(include=[np.number]).columns
numeric_cols = numeric_cols.drop('SalePrice')  # Exclude the target column
categorical_cols = train_df.select_dtypes(exclude=[np.number]).columns

# Fill missing values in train set
train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())
train_df[categorical_cols] = train_df[categorical_cols].fillna(train_df[categorical_cols].mode().iloc[0])

# Fill missing values in test set
test_df[numeric_cols] = test_df[numeric_cols].fillna(test_df[numeric_cols].median())
test_df[categorical_cols] = test_df[categorical_cols].fillna(test_df[categorical_cols].mode().iloc[0])

# Encode categorical variables
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    train_df[col] = le.fit_transform(train_df[col].astype(str))
    test_df[col] = le.transform(test_df[col].astype(str))
    label_encoders[col] = le

# Print the processed data
print(train_df.head())
print(test_df.head())

# Separate features (X) and target (y)
X = train_df.drop('SalePrice', axis=1)
y = train_df['SalePrice']

# Split data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[numeric_cols])  # Scale only numeric features
X_val_scaled = scaler.transform(X_val[numeric_cols])
X_test_scaled = scaler.transform(test_df[numeric_cols])

# Function to calculate regression metrics
def regression_metrics(y_true, y_pred):
    metrics = {
        'R-squared': r2_score(y_true, y_pred),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MSE': mean_squared_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred))
    }
    return metrics

# Define and train various regression models
regression_models = {
    'Linear Regression': LinearRegression(),
    'KNN Regressor': KNeighborsRegressor(),
    'SVM Regressor': SVR(),
    'Decision Tree Regressor': DecisionTreeRegressor(),
    'Random Forest Regressor': RandomForestRegressor(),
    'Gradient Boosting Regressor': GradientBoostingRegressor(),
    'Neural Network Regressor': MLPRegressor(max_iter=1000)
}
# Dictionary to store the results
house_price_results = {}

# Loop through models, train, and evaluate them
for name, model in regression_models.items():
    # Train the model
    model.fit(X_train_scaled, y_train)

    # Make predictions (for evaluation on validation set)
    y_pred = model.predict(X_val_scaled)

    # Store the performance metrics
    house_price_results[name] = regression_metrics(y_val, y_pred)

# Convert the results to a DataFrame for better visualization
house_price_results_df = pd.DataFrame(house_price_results).T
print("\nModel Performance Metrics for House Price Prediction:")
print(house_price_results_df)

import matplotlib.pyplot as plt
# Set up the figure and axes for the plots
fig, axs = plt.subplots(2, 2, figsize=(15, 10))

# Plot R-squared values
axs[0, 0].barh(house_price_results_df.index, house_price_results_df['R-squared'], color='skyblue')
axs[0, 0].set_title('R-squared Values of Regression Models')
axs[0, 0].set_xlabel('R-squared')

# Plot Mean Absolute Error
axs[0, 1].barh(house_price_results_df.index, house_price_results_df['MAE'], color='salmon')
axs[0, 1].set_title('Mean Absolute Error of Regression Models')
axs[0, 1].set_xlabel('MAE')

# Plot Mean Squared Error
axs[1, 0].barh(house_price_results_df.index, house_price_results_df['MSE'], color='lightgreen')
axs[1, 0].set_title('Mean Squared Error of Regression Models')
axs[1, 0].set_xlabel('MSE')

# Plot Root Mean Squared Error
axs[1, 1].barh(house_price_results_df.index, house_price_results_df['RMSE'], color='gold')
axs[1, 1].set_title('Root Mean Squared Error of Regression Models')
axs[1, 1].set_xlabel('RMSE')

# Adjust layout
plt.tight_layout()
plt.show()

import seaborn as sns

#Bar Plot for R-squared values
plt.figure(figsize=(10, 6))
plt.barh(house_price_results_df.index, house_price_results_df['R-squared'], color='teal')
plt.title('R-squared Values of Regression Models')
plt.xlabel('R-squared')
plt.tight_layout()
plt.show()

#Box Plot for MAE, MSE, and RMSE
plt.figure(figsize=(10, 6))
sns.boxplot(data=house_price_results_df[['MAE', 'MSE', 'RMSE']], palette="Set2")
plt.title('Box Plot of MAE, MSE, and RMSE')
plt.ylabel('Error Metrics')
plt.tight_layout()
plt.show()

#Radar Chart for all metrics
labels = house_price_results_df.index  # Model names
metrics = ['R-squared', 'MAE', 'MSE', 'RMSE']  # Metrics to plot
num_vars = len(metrics)

# Create a radar chart
angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]  # Close the loop for angles

# Set up the radar plot
fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

# Iterate over each model's metrics
for i in range(len(labels)):
    # Get the stats for each model
    stats = house_price_results_df.iloc[i][metrics].values.flatten().tolist()
    stats += stats[:1]

ax.fill(angles, stats, alpha=0.25)
ax.plot(angles, stats, label=labels[i])
ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(metrics, fontsize=12)
ax.set_title('Radar Chart for Regression Models', size=15)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

#Line Plot for MAE, MSE, RMSE
plt.figure(figsize=(10, 6))
plt.plot(house_price_results_df.index, house_price_results_df['MAE'], marker='o', label='MAE', color='orange')
plt.plot(house_price_results_df.index, house_price_results_df['MSE'], marker='o', label='MSE', color='blue')
plt.plot(house_price_results_df.index, house_price_results_df['RMSE'], marker='o', label='RMSE', color='green')
plt.title('Line Plot of Error Metrics by Model')
plt.xlabel('Regression Models')
plt.ylabel('Error Metrics')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Model Performance with More Features:

Again, the Gradient Boosting Regressor achieves the best performance on the House Price dataset with the highest R-squared value of 0.893896 and the lowest MAE, MSE, and RMSE. This suggests it is the most effective model for this complex dataset.
The Random Forest Regressor also performs exceptionally well, but it shows slightly higher error metrics compared to the Gradient Boosting Regressor.
Notably, the SVM Regressor and Neural Network Regressor underperform in the House Price dataset, particularly the Neural Network, which yields negative R-squared values indicating poor predictive performance.
"""